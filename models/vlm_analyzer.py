import logging
import torch
import numpy as np
import cv2
import os
import math
from typing import Dict, Any, List, Tuple
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, GPT2Tokenizer
from concurrent.futures import ProcessPoolExecutor
from PIL import Image

# ç¡®ä¿æ—¥å¿—é…ç½®æ­£ç¡®
logger = logging.getLogger(__name__)


class VLMSceneAnalyzer:
    """
    è´Ÿè´£åŠ è½½ ViT-GPT2 æ¨¡åž‹ã€æå–å…³é”®å¸§å’Œæ‰¹é‡ç”Ÿæˆåœºæ™¯æè¿°ã€‚
    è¿™æ˜¯ç‹¬ç«‹çš„ VLM ç»„ä»¶ï¼Œè´Ÿè´£æ‰€æœ‰è§†è§‰åˆ†æžå·¥ä½œã€‚
    """

    def __init__(self):
        self.vit_gpt2_model = None
        self.vit_processor = None
        self.gpt2_tokenizer = None
        # VLM è®¾å¤‡è®¾ç½®
        self.vlm_device = "cuda" if torch.cuda.is_available() else "cpu"
        # ä¼˜åŒ–å‚æ•°
        self.frame_size = (224, 224)  # ViT-GPT2çš„æœ€ä½³è¾“å…¥å°ºå¯¸
        self.min_frame_interval = 1.0  # å…³é”®å¸§ä¹‹é—´çš„æœ€å°æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰

        self.load_model()

    def load_model(self):
        """åŠ è½½ ViT-GPT2 æ¨¡åž‹åŠå…¶ç»„ä»¶ã€‚"""
        vlm_model_id = "nlpconnect/vit-gpt2-image-captioning"
        logger.info(f"Initializing ViT-GPT2 model '{vlm_model_id}' on device: {self.vlm_device}")
        try:
            self.vit_processor = ViTImageProcessor.from_pretrained(vlm_model_id)
            self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained(vlm_model_id)

            # ä½¿ç”¨ fp16 ä¼˜åŒ– VRAM (ä»…é™ CUDA)
            vlm_dtype = torch.float16 if self.vlm_device == "cuda" else torch.float32

            self.vit_gpt2_model = VisionEncoderDecoderModel.from_pretrained(
                vlm_model_id,
                torch_dtype=vlm_dtype
            ).to(self.vlm_device)

            if self.gpt2_tokenizer.pad_token is None:
                self.gpt2_tokenizer.pad_token = self.gpt2_tokenizer.eos_token
                logger.warning("Set pad_token to eos_token for GPT2Tokenizer")
            logger.info("âœ… ViT-GPT2 model and components loaded successfully.")
        except Exception as e:
            logger.error(f"ViT-GPT2 load failed: {e}")
            self.vit_gpt2_model = None
        finally:
            if self.vlm_device == "cuda":
                torch.cuda.empty_cache()

    # --- åœºæ™¯è§£æžè¾…åŠ©å‡½æ•° (ä¸­æ–‡è§£æžé€»è¾‘) ---

    def _parse_environment(self, desc: str) -> str:
        """æ ¹æ®æè¿°è§£æžçŽ¯å¢ƒ/åœºæ‰€ç±»åž‹ (ä¸­æ–‡)ã€‚"""
        special_places = {
            "ç›‘ç‹±": ["prison", "jail", "cell", "inmate", "correctional facility", "guard", "bars"],
            "è­¦å¯Ÿå±€": ["police station", "police office", "cop shop", "detention center", "police car", "officer"],
            "åŒ»é™¢": ["hospital", "clinic", "medical center", "ward", "emergency room", "doctor", "nurse", "patient",
                     "bed"],
            "å•†åº—": ["store", "shop", "market", "mall", "retail", "counter", "customer", "product"],
            "å­¦æ ¡": ["school", "classroom", "university", "college", "student", "teacher", "desk", "blackboard"],
            "åŠžå…¬å®¤": ["office", "workplace", "desk", "computer", "employee", "meeting room", "cubicle"],
            "é¤åŽ…": ["restaurant", "cafe", "diner", "table", "chair", "menu", "waiter", "food"],
            "é…’åº—": ["hotel", "motel", "lobby", "room", "reception", "guest"],
            "é“¶è¡Œ": ["bank", "teller", "ATM", "vault", "customer service"],
            "æœºåœº": ["airport", "terminal", "plane", "gate", "passenger", "luggage"],
            "è½¦ç«™": ["train station", "bus station", "platform", "ticket", "passenger"],
            "å›¾ä¹¦é¦†": ["library", "book", "shelf", "reader", "desk"],
            "åšç‰©é¦†": ["museum", "exhibit", "artifact", "display", "visitor"],
            "ä½“è‚²é¦†": ["stadium", "gym", "court", "field", "player", "audience"],
            "ç”µå½±é™¢": ["cinema", "theater", "movie", "screen", "seat", "audience"],
            "æ•™å ‚": ["church", "temple", "mosque", "prayer", "worship", "altar"],
        }
        outdoor_scenes = {
            "åŸŽå¸‚è¡—é“": ["street", "road", "car", "traffic", "building", "sidewalk", "crosswalk", "traffic light"],
            "å…¬å›­": ["park", "garden", "tree", "flower", "bench", "path", "playground"],
            "æ£®æž—": ["forest", "woods", "tree", "leaf", "animal", "trail"],
            "æµ·æ»©": ["beach", "sand", "ocean", "sea", "wave", "umbrella", "swimmer"],
            "å±±è„‰": ["mountain", "hill", "peak", "valley", "hiker", "trail"],
            "ç”°é‡Ž": ["field", "farm", "crop", "tractor", "farmer", "grass"],
            "å·¥åœ°": ["construction site", "worker", "crane", "building", "material"],
            "åœè½¦åœº": ["parking lot", "car", "parking space", "vehicle"],
            "åŠ æ²¹ç«™": ["gas station", "fuel", "pump", "car", "attendant"],
        }
        indoor_scenes = {
            "å®¶åº­ä½å®…": ["house", "home", "living room", "kitchen", "bedroom", "bathroom", "sofa", "TV"],
            "å…¬å¯“": ["apartment", "flat", "living room", "kitchen", "bedroom", "tenant"],
            "å®¿èˆ": ["dormitory", "dorm", "room", "student", "bed", "desk"],
        }
        urban_keywords = ["city", "urban", "building", "street", "car", "traffic", "skyscraper", "apartment"]
        rural_keywords = ["countryside", "rural", "farm", "field", "village", "cottage", "tractor", "animal"]

        desc_lower = desc.lower()
        for place, keywords in special_places.items():
            if any(kw in desc_lower for kw in keywords): return place
        for scene, keywords in outdoor_scenes.items():
            if any(kw in desc_lower for kw in keywords): return scene
        for scene, keywords in indoor_scenes.items():
            if any(kw in desc_lower for kw in keywords): return scene
        if any(kw in desc_lower for kw in urban_keywords): return "åŸŽå¸‚åŒºåŸŸ"
        if any(kw in desc_lower for kw in rural_keywords): return "å†œæ‘åŒºåŸŸ"
        if any(kw in desc_lower for kw in ["indoor", "inside", "room", "building"]): return "å®¤å†…åœºæ‰€"
        if any(kw in desc_lower for kw in ["outdoor", "outside", "open area"]): return "å®¤å¤–åœºæ‰€"
        return "æœªçŸ¥åœºæ‰€"

    def _parse_emotion(self, desc: str) -> str:
        """æ ¹æ®æè¿°è§£æžäººç‰©æƒ…ç»ª (ä¸­æ–‡)ã€‚"""
        positive_keywords = ["smiling", "happy", "laughing", "excited", "joyful", "cheerful", "grinning", "delighted"]
        calm_keywords = ["calm", "relaxed", "quiet", "still", "peaceful", "serene", "composed"]
        negative_keywords = ["sad", "angry", "upset", "frowning", "frustrated", "crying", "mad", "serious"]
        desc_lower = desc.lower()
        if any(kw in desc_lower for kw in positive_keywords): return "å¼€å¿ƒ/å…´å¥‹"
        if any(kw in desc_lower for kw in calm_keywords): return "å¹³é™/æ”¾æ¾"
        if any(kw in desc_lower for kw in negative_keywords): return "æ‚²ä¼¤/æ„¤æ€’/ä¸¥è‚ƒ"
        return "ä¸­æ€§"

    def _parse_activity(self, desc: str) -> str:
        """æ ¹æ®æè¿°è§£æžäººç‰©æ´»åŠ¨ (ä¸­æ–‡)ã€‚"""
        talking_keywords = ["talking", "speaking", "discussing", "interview", "chatting", "conversing", "explaining"]
        action_keywords = ["holding", "using", "playing", "running", "walking", "skateboarding", "dancing", "eating",
                           "drinking", "writing", "reading", "gaming", "playing a game"]
        static_keywords = ["standing", "sitting", "posing", "looking", "watching", "listening", "sleeping", "resting"]
        desc_lower = desc.lower()
        if any(kw in desc_lower for kw in talking_keywords): return "äº¤è°ˆ/è¯´è¯/è§£è¯´"
        if any(kw in desc_lower for kw in action_keywords): return "è¿›è¡ŒåŠ¨ä½œï¼ˆæŒç‰©/è¿åŠ¨/æ¸¸æˆç­‰ï¼‰"
        if any(kw in desc_lower for kw in static_keywords): return "é™æ­¢çŠ¶æ€ï¼ˆç«™ç«‹/åå§¿ç­‰ï¼‰"
        return "æœªçŸ¥æ´»åŠ¨"

    def _parse_scene_type(self, desc: str) -> str:
        """æ ¹æ®æè¿°è§£æžåœºæ™¯ç±»åž‹ (ä¸­æ–‡)ã€‚"""
        live_stream_keywords = ["live", "stream", "streamer", "ä¸»æ’­", "ç›´æ’­", "è§£è¯´", "commentary", "ui", "interface",
                                "å¼¹å¹•", "danmu", "chat", "èŠå¤©", "ç¤¼ç‰©", "å…³æ³¨", "ç‚¹èµž"]
        game_keywords = ["game", "gaming", "video game", "character", "è§’è‰²", "player", "çŽ©å®¶", "level", "åœ°å›¾", "map",
                         "quest", "ä»»åŠ¡", "hp", "mp", "health", "mana", "score", "å¾—åˆ†", "loading", "menu", "inventory",
                         "è£…å¤‡", "weapon", "æ­¦å™¨", "æ•Œäºº", "boss", "æˆ˜æ–—", "æˆ˜æ–—åœºæ™¯", "åƒç´ ", "pixel",
                         "3d render", "animated"]
        desc_lower = desc.lower()
        if any(kw in desc_lower for kw in live_stream_keywords):
            return "æ¸¸æˆç›´æ’­è§£è¯´ç”»é¢" if any(kw in desc_lower for kw in game_keywords) else "ç›´æ’­è§£è¯´ç”»é¢"
        if any(kw in desc_lower for kw in game_keywords): return "æ¸¸æˆç”»é¢"
        return "çœŸå®žä¸–ç•Œåœºæ™¯"

    # --- å¸§æå–å’Œ VLM æŽ¨ç†æ ¸å¿ƒå‡½æ•° ---

    @staticmethod
    def _extract_frames_worker(video_path: str, timestamps: List[float], frame_size: Tuple[int, int]) -> Dict[
        float, np.ndarray]:
        """
        ã€å¤šè¿›ç¨‹å·¥ä½œå•å…ƒã€‘ä»Žè§†é¢‘ä¸­æå–æŒ‡å®šæ—¶é—´æˆ³çš„å¸§ï¼Œå¹¶è¿›è¡Œå°ºå¯¸ç¼©æ”¾ã€‚
        """
        frame_cache = {}
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            logger.error(f"Failed to open video file: {video_path}")
            return frame_cache

        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        for ts in timestamps:
            frame_idx = int(ts * fps)
            frame_idx = min(max(0, frame_idx), total_frames - 1)

            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if ret:
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frame_resized = cv2.resize(frame_rgb, frame_size)
                frame_cache[ts] = frame_resized

        cap.release()
        return frame_cache

    def _deduplicate_timestamps(self, timestamps: List[float], final_limit: int, duration: float) -> List[float]:
        """
        å¯¹å…³é”®å¸§æ—¶é—´æˆ³è¿›è¡ŒåŽ»é‡ï¼Œç¡®ä¿åœ¨æŒ‡å®šçš„æœ€å°é—´éš”å†…åªä¿ç•™ä¸€ä¸ªï¼Œå¹¶ä½¿ç”¨æœ€ç»ˆæ•°é‡é™åˆ¶è¿›è¡Œå‡åŒ€é‡‡æ ·ã€‚
        """
        if not timestamps: return []

        # 1. æœ€å°é—´éš”åŽ»é‡
        sorted_ts = sorted(timestamps)
        deduplicated = [sorted_ts[0]]
        for ts in sorted_ts[1:]:
            if ts - deduplicated[-1] >= self.min_frame_interval:
                deduplicated.append(ts)

        original_count = len(sorted_ts)
        dedup_count = len(deduplicated)

        logger.info(
            f"ðŸŽ¬ VLM Frame Sampling: Original {original_count} frames -> Deduplicated {dedup_count} frames"
        )

        # 2. å‡åŒ€é‡‡æ ·é™åˆ¶
        if dedup_count > final_limit:
            indices = np.linspace(0, dedup_count - 1, final_limit, dtype=int)
            deduplicated = [deduplicated[i] for i in indices]

            final_count = len(deduplicated)
            logger.info(
                f"âž¡ï¸ VLM Final Sampling: Exceeded limit ({final_limit} frames), uniformly sampled {final_count} frames"
            )

        return deduplicated

    def _process_frames_batch(self, frames_data: List[Tuple[float, np.ndarray]]) -> List[Dict[str, Any]]:
        """
        ã€ä¸»è¿›ç¨‹æ‰§è¡Œã€‘æ‰¹é‡å¤„ç†å¸§æ•°æ®ï¼Œç”Ÿæˆåœºæ™¯æè¿°ï¼Œå¹¶è¿›è¡Œè§£æžã€‚
        """
        if not frames_data or self.vit_gpt2_model is None:
            logger.warning("VLM model is not loaded, skipping batch processing.")
            return []

        timestamps, frames = zip(*frames_data)
        vlm_dtype = torch.float16 if self.vlm_device == "cuda" else torch.float32

        # æ‰¹é‡é¢„å¤„ç† (frames æ˜¯ np.ndarray åˆ—è¡¨)
        pixel_values = self.vit_processor(
            images=[Image.fromarray(f) for f in frames],
            return_tensors="pt",
        ).pixel_values.to(self.vlm_device, dtype=vlm_dtype)

        # æ‰¹é‡ç”Ÿæˆæè¿°
        with torch.no_grad():
            gen_ids = self.vit_gpt2_model.generate(
                pixel_values,
                max_length=100,
                num_beams=4,
                early_stopping=True,
                no_repeat_ngram_size=2,
                pad_token_id=self.gpt2_tokenizer.pad_token_id,
                eos_token_id=self.gpt2_tokenizer.eos_token_id
            )

        raw_descriptions = self.gpt2_tokenizer.batch_decode(gen_ids, skip_special_tokens=True)

        # æ‰¹é‡è§£æžç»“æžœ
        results = []
        for ts, desc in zip(timestamps, raw_descriptions):
            desc_stripped = desc.strip()
            result = {
                "timestamp": round(ts, 2),
                "description": desc_stripped,
                "scene_type": self._parse_scene_type(desc_stripped),
                "environment": self._parse_environment(desc_stripped),
                "emotion": self._parse_emotion(desc_stripped),
                "activity": self._parse_activity(desc_stripped),
            }
            results.append(result)

            logger.info(
                f"ðŸ–¼ï¸ Frame Context Analysis (TS: {result['timestamp']}s): "
                f"[{result['scene_type']}/{result['environment']}] "
                f"Emotion: {result['emotion']}, "
                f"Activity: {result['activity']}. "
                f"Description: '{result['description']}'"
            )

        return results

    def analyze_frames(self, video_path: str, target_timestamps: List[float]) -> Dict[float, Dict[str, Any]]:
        """
        Executes frame extraction (multi-process) and VLM inference (main process).
        Returns a map from timestamp to scene context.
        """
        frame_ctx_cache = {}
        if self.vit_gpt2_model is None:
            logger.error("VLM model is not available. Cannot analyze frames.")
            return {}

        # 1. Multi-process frame extraction (I/O)
        frame_cache = {}
        try:
            logger.info(f"Entering ProcessPoolExecutor, preparing to extract {len(target_timestamps)} frames...")

            with ProcessPoolExecutor(max_workers=min(4, os.cpu_count() or 1)) as executor:
                extract_future = executor.submit(
                    VLMSceneAnalyzer._extract_frames_worker,
                    video_path,
                    target_timestamps,
                    self.frame_size
                )
                frame_cache = extract_future.result()

            logger.info(f"ProcessPoolExecutor exited, successfully extracted {len(frame_cache)} frames.")

            if not frame_cache:
                logger.error("Failed to extract any frames in the multi-process pool.")
                return {}

            # 2. Main process batch VLM inference (GPU)
            frames_to_process = sorted(frame_cache.items(), key=lambda item: item[0])
            batch_size = 16
            frame_batches = [frames_to_process[i:i + batch_size] for i in
                             range(0, len(frames_to_process), batch_size)]

            logger.info(f"Starting VLM batch inference ({len(frames_to_process)} frames, Batch={batch_size})...")

            for i, batch in enumerate(frame_batches):
                logger.info(f"Processing VLM batch {i + 1}/{len(frame_batches)}")
                batch_results = self._process_frames_batch(batch)
                for res in batch_results:
                    frame_ctx_cache[res.pop("timestamp")] = res

            logger.info(f"VLM inference completed, generated {len(frame_ctx_cache)} valid descriptions.")
            return frame_ctx_cache

        except Exception as e:
            logger.error(f"Video scene analysis failed: {e}", exc_info=True)
            return {}
        finally:
            if self.vlm_device == "cuda":
                torch.cuda.empty_cache()