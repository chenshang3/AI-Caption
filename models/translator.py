import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import logging
import gc
from typing import Optional, Dict, Any, List
from sentence_transformers import SentenceTransformer, util
import re
import os
os.makedirs("offload_nllb", exist_ok=True)
logger = logging.getLogger(__name__)


class NeuralTranslator:
    def __init__(self, nmt_model_id="facebook/nllb-200-distilled-600M", reflection_model_id=None, device='cpu'):
        # --- ä¼˜åŒ–ç‚¹ï¼šè‡ªåŠ¨æ£€æµ‹å¹¶è®¾ç½® GPU è®¾å¤‡ ---
        self.device = torch.device("cuda" if torch.cuda.is_available() and device == 'cuda' else 'cpu')
        self.nmt_tokenizer = None
        self.nmt_model = None
        self.reflector = None
        self.qe_model = None
        self.qe_threshold = 0.7

        self.nmt_max_length = 150
        self.nmt_max_input_length = 128

        self._load_models(nmt_model_id, reflection_model_id)

    def _load_models(self, nmt_model_id: str, reflection_model_id: Optional[str]):
        """åŠ è½½ NMT æ¨¡å‹ã€åæ€æ¨¡å‹å’Œ QE æ¨¡å‹"""
        try:
            # 1. åŠ è½½ NMT æ¨¡å‹ï¼ˆNLLBï¼Œæ”¯æŒå¤šè¯­è¨€ç¿»è¯‘ï¼‰
            local_path = "models/nllb-200-3.3B"
            logger.info(f"Loading NMT model: {nmt_model_id} (Device: {self.device})")
            logger.info(f"Loading NLLB model from local path: {local_path}")
            self.nmt_tokenizer = AutoTokenizer.from_pretrained(
            local_path,
            local_files_only=True           # ç¦æ­¢è®¿é—® huggingface
)
            # ä½¿ç”¨ float16 å‡å°‘æ˜¾å­˜å ç”¨ï¼ŒåŠ é€Ÿ GPU æ¨ç†
            nmt_dtype = torch.float16 if self.device.type == 'cuda' else torch.float32

            # [æ³¨æ„] transformers åº“æç¤º torch_dtype å·²åºŸå¼ƒï¼Œä½¿ç”¨ dtype
            self.nmt_model = AutoModelForSeq2SeqLM.from_pretrained(
            local_path,
            use_safetensors=True,
            low_cpu_mem_usage=True,
            dtype=nmt_dtype,
            device_map="auto",
            offload_folder="offload_nllb",
            local_files_only=True           # â­ å…³é”®ï¼šåªåŠ è½½æœ¬åœ°ï¼Œä¸è”ç½‘
).to(self.device)
            logger.info("âœ… NMT model loaded successfully.")

            # 2. åŠ è½½åæ€æ¨¡å‹ï¼ˆå¯é€‰ï¼Œç”¨äºä¼˜åŒ–ç¿»è¯‘ç»“æœï¼‰
            if reflection_model_id:
                logger.info(f"Loading reflection model: {reflection_model_id}")
                reflector_device_index = 0 if self.device.type == 'cuda' else -1
                reflection_dtype = torch.float16 if self.device.type == 'cuda' else torch.float32

                self.reflector = pipeline(
                "text-generation",
                model="/home/kou000/AI-Caption/models/Qwen3-8B",
                torch_dtype=torch.float32,
                device_map="auto",
                model_kwargs={
                    "low_cpu_mem_usage": True,
                    "use_safetensors": True
                })


                # --- å…³é”®ä¿®å¤ç‚¹/å¼ºåŒ–ç‚¹ï¼šç¡®ä¿ reflector çš„ pad_token è®¾ç½®ç¨³å®š ---
                if self.reflector.tokenizer.pad_token is None and self.reflector.tokenizer.eos_token is not None:
                    self.reflector.tokenizer.pad_token = self.reflector.tokenizer.eos_token
                    # ç¡®ä¿ model config ä¹Ÿæ›´æ–°ï¼Œè¿™å¯¹äºåç»­çš„ generate è°ƒç”¨æ˜¯å…³é”®
                    self.reflector.model.config.pad_token_id = self.reflector.tokenizer.eos_token_id
                    logger.warning(f"Set pad_token/id to eos_token/id for reflection model.")
                # -----------------------------------------------------------

                logger.info("âœ… Reflection model loaded successfully.")
            else:
                logger.warning("No reflection model specified, skipping optimization.")

            # 3. åŠ è½½ QE æ¨¡å‹ï¼ˆå¥å­ç›¸ä¼¼åº¦æ¨¡å‹ï¼Œç”¨äºè¯„ä¼°ç¿»è¯‘è´¨é‡ï¼‰
            self._load_qe_model()

        except Exception as e:
            logger.error(f"Model load failed: {e}", exc_info=True)
            self._cleanup_vram()
            raise Exception(f"Translator init error: {str(e)}")

    def _load_qe_model(self):
        """åŠ è½½ç¿»è¯‘è´¨é‡è¯„ä¼°ï¼ˆQEï¼‰æ¨¡å‹"""
        try:
            self.qe_model = SentenceTransformer("all-MiniLM-L6-v2", device=self.device.type)
            logger.info("âœ… QE model (sentence-transformers) loaded successfully.")
        except Exception as e:
            logger.warning(f"QE model load failed: {e}", exc_info=True)
            self.qe_model = None

    def translate_segments(self, segments: List[Dict[str, Any]], target_lang: str, source_lang: str = 'auto',
                           use_reflection: bool = False, av_context: Optional[Dict[str, Any]] = None) -> List[
        Dict[str, Any]]:
        """
        ç¿»è¯‘å­—å¹•ç‰‡æ®µï¼ˆæ”¯æŒç‰‡æ®µçº§ AV ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼‰
        """
        for idx, seg in enumerate(segments):
            if not all(key in seg for key in ["start", "end", "text"]):
                raise ValueError(f"Segment {idx + 1} missing required fields (start/end/text).")
            if "av_context" not in seg:
                seg["av_context"] = av_context or {}
                logger.debug(f"Segment {idx + 1} has no av_context, using global context.")

        source_texts = [seg["text"].strip() for seg in segments]
        logger.info(
            f"Starting translation: {len(source_texts)} segments -> Target lang: {target_lang} (Reflection: {use_reflection})")

        # ç¬¬ä¸€æ­¥ï¼šæ‰¹é‡ç¿»è¯‘ï¼ˆåŸºç¡€ç¿»è¯‘ç»“æœï¼‰
        translated_texts = self._translate_batch(source_texts, source_lang, target_lang)
        logger.info(f"Batch translation completed.")

        # ç¬¬äºŒæ­¥ï¼šåæ€ä¼˜åŒ–ï¼ˆé€ç‰‡æ®µç»“åˆè‡ªèº« AV ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼‰
        if use_reflection and self.reflector:
            logger.info("Starting reflection optimization with segment-level AV context...")
            optimized_texts = []

            for idx, (seg, src_text, trans_text) in enumerate(zip(segments, source_texts, translated_texts)):
                segment_av_ctx = seg["av_context"] or av_context or {}
                optimized = self._reflect_and_improve(src_text, trans_text, target_lang, segment_av_ctx, idx + 1)
                optimized_texts.append(optimized)
            translated_texts = optimized_texts
            logger.info("Reflection optimization completed.")

        # ç¬¬ä¸‰æ­¥ï¼šè®¡ç®— QE åˆ†æ•°ï¼ˆè¯„ä¼°ç¿»è¯‘è´¨é‡ï¼‰
        qe_scores = self._calculate_batch_qe_scores(source_texts, translated_texts) if self.qe_model else [0.0] * len(
            source_texts)

        # ç¬¬å››æ­¥ï¼šç»„è£…æœ€ç»ˆç»“æœ
        result = []
        for idx, (seg, trans_text, qe_score) in enumerate(zip(segments, translated_texts, qe_scores)):
            result.append({
                "start": round(seg["start"], 2),
                "end": round(seg["end"], 2),
                "text": trans_text,
                "original_text": seg["text"],
                "qe_score": round(qe_score, 2),
                "av_context": seg["av_context"],
                "is_optimized": use_reflection and self.reflector is not None
            })

        logger.info(f"Translation process finished: {len(result)} segments processed.")

        return result

    def _translate_batch(self, texts: List[str], src_lang_code: str, tgt_lang_code: str) -> List[str]:
        """
        æ‰¹é‡ç¿»è¯‘æ–‡æœ¬ï¼ˆNLLB æ¨¡å‹æ ¸å¿ƒç¿»è¯‘é€»è¾‘ï¼‰- ä¼˜åŒ–äº†æ‰¹é‡å¤„ç†æ•ˆç‡
        """
        lang_map = {
            'auto': 'eng_Latn', 'en': 'eng_Latn', 'zh': 'zho_Hans', 'zh-cn': 'zho_Hans',
            'ja': 'jpn_Jpan', 'ko': 'kor_Hang', 'fr': 'fra_Latn', 'de': 'deu_Latn',
            'es': 'spa_Latn', 'ru': 'rus_Cyrl', 'ar': 'ara_Arab', 'hi': 'hin_Deva',
            'pt': 'por_Latn', 'it': 'ita_Latn', 'nl': 'nld_Latn', 'pl': 'pol_Latn'
        }

        src_code = lang_map.get(src_lang_code.lower(), 'eng_Latn')
        tgt_code = lang_map.get(tgt_lang_code.lower(), 'zho_Hans')
        logger.debug(f"Batch translation: src_code={src_code}, tgt_code={tgt_code}, text_count={len(texts)}")

        inputs = self.nmt_tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.nmt_max_input_length
        ).to(self.device)

        forced_bos_token_id = self.nmt_tokenizer.convert_tokens_to_ids(tgt_code)

        with torch.no_grad():
            generated_tokens = self.nmt_model.generate(
                **inputs,
                forced_bos_token_id=forced_bos_token_id,
                max_length=self.nmt_max_length,
                num_beams=4,
                do_sample=False,
                early_stopping=True,
                no_repeat_ngram_size=2
            )

        translations = self.nmt_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
        translations = [trans.strip() for trans in translations]

        return translations

    def _reflect_and_improve(self, source_text: str, initial_translation: str, tgt_lang: str,
                             segment_av_ctx: Dict[str, Any], segment_idx: int) -> str:
        """
        ç»“åˆç‰‡æ®µçº§ AV ä¸Šä¸‹æ–‡ä¼˜åŒ–ç¿»è¯‘ç»“æœï¼ˆåæ€æœºåˆ¶ï¼‰
        """
        scene_type = segment_av_ctx.get("scene_type", "æ— /æœªçŸ¥").strip()
        environment = segment_av_ctx.get("environment", "æ— /æœªçŸ¥").strip()
        emotion = segment_av_ctx.get("emotion", "æ— /æœªçŸ¥").strip()
        activity = segment_av_ctx.get("activity", "æ— /æœªçŸ¥").strip()
        scene_desc = segment_av_ctx.get("description", "æ— è¯¦ç»†æè¿°").strip()

        # æ„å»ºåæ€æç¤ºè¯ï¼ˆç§»é™¤å‰ç¼€ï¼Œç›´æ¥è¦æ±‚è¾“å‡ºæ–‡æœ¬ï¼‰
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ã€åœºæ™¯æ„ŸçŸ¥çš„å­—å¹•ç¿»è¯‘åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„**æ‰€æœ‰**åœºæ™¯ä¿¡æ¯å’Œè§†è§‰æè¿°ï¼Œä¼˜åŒ–ç»™å®šçš„ç¿»è¯‘ç»“æœï¼Œä»¥ç¡®ä¿ç¿»è¯‘çš„è¯æ±‡ã€é£æ ¼å’Œæƒ…æ„Ÿä¸åœºæ™¯é«˜åº¦åŒ¹é…ã€‚

=== å½“å‰åœºæ™¯ä¿¡æ¯ (æ¥è‡ª VLM çš„æå–ç»“æœ) ===
- åœºæ™¯ç±»å‹: {scene_type}
- å…·ä½“ç¯å¢ƒ: {environment}
- äººç‰©æƒ…æ„Ÿ: {emotion}
- æ´»åŠ¨çŠ¶æ€: {activity}
- è¯¦ç»†è§†è§‰æè¿°: {scene_desc}

=== ç¿»è¯‘å®ˆåˆ™ ===
1. ã€å‡†ç¡®æ€§ã€‘ç¿»è¯‘å¿…é¡»ä¸¥æ ¼å¿ äºåŸæ–‡å«ä¹‰ï¼Œä¸å¾—å¢åˆ è¯­ä¹‰ã€‚
2. ã€åœºæ™¯é€‚é…ã€‘è¯·æ ¹æ®**ä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯**ï¼Œé€‰æ‹©æœ€ç¬¦åˆåœºæ™¯ï¼ˆä¾‹å¦‚ï¼šæ¸¸æˆã€ç›´æ’­ã€åŒ»ç–—ã€æ³•å¾‹ã€æ—¥å¸¸ç”Ÿæ´»ç­‰ï¼‰çš„ä¸“ä¸šæœ¯è¯­å’Œå£è¯­åŒ–é£æ ¼ã€‚
3. ã€æƒ…æ„ŸåŒ¹é…ã€‘ç¿»è¯‘ç»“æœåº”èƒ½åæ˜ äººç‰©çš„æƒ…æ„ŸçŠ¶æ€ï¼ˆä¾‹å¦‚ï¼šå…´å¥‹ã€å¹³é™ã€ä¸¥è‚ƒï¼‰ã€‚
4. ã€ç®€æ´æ€§ã€‘å­—å¹•éœ€ç®€çŸ­ç²¾ç‚¼ï¼Œæ˜“äºè§‚ä¼—å¿«é€Ÿé˜…è¯»ã€‚
5. ã€ç›®æ ‡è¯­è¨€ã€‘è¯·ç¿»è¯‘æˆ{self._get_lang_name(tgt_lang)}ã€‚

=== éœ€è¦ä¼˜åŒ–çš„å†…å®¹ ===
- æºæ–‡æœ¬: "{source_text}"
- åŸºç¡€ç¿»è¯‘: "{initial_translation}"

è¯·æ ¹æ®ä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œè¾“å‡ºä¼˜åŒ–åçš„æœ€ç»ˆç¿»è¯‘ç»“æœã€‚
**ğŸ”´ æ ¸å¿ƒæŒ‡ä»¤: ä½ çš„å›ç­”ä¸­ï¼Œå¿…é¡»ä¸”åªèƒ½åŒ…å«æœ€ç»ˆä¼˜åŒ–åçš„çº¯å‡€ä¸­æ–‡å­—å¹•æ–‡æœ¬ï¼Œä¸å…è®¸åŒ…å«ä»»ä½•è§£é‡Šã€åˆ†æã€æ‰“æ‹›å‘¼æˆ–é¢å¤–çš„æ–‡å­—ã€‚è¯·ç«‹å³å¼€å§‹è¾“å‡ºç¿»è¯‘æ–‡æœ¬ã€‚**
"""  # æç¤ºè¯æœ«å°¾æ²¡æœ‰å¤šä½™çš„å­—ç¬¦æˆ–å‰ç¼€

        try:
            # ç¡®ä¿ä½¿ç”¨ prompt_template è€Œä¸æ˜¯ self.prompt
            prompt_text = prompt.strip()

            # åœ¨ pipeline è°ƒç”¨ä¸­ï¼Œæ˜¾å¼ä¼ é€’ pad_token_id å’Œ eos_token_id æ¥ä¿è¯ç”Ÿæˆç¨³å®šæ€§
            pad_id = self.reflector.model.config.pad_token_id
            eos_id = self.reflector.model.config.eos_token_id

            response = self.reflector(
                prompt_text,
                max_new_tokens=150,
                do_sample=False,
                num_return_sequences=1,
                # ç¡®ä¿ generate è°ƒç”¨ä½¿ç”¨çš„å‚æ•°æ˜¯ç¨³å®šçš„
                pad_token_id=pad_id,
                eos_token_id=eos_id,
            )[0]["generated_text"]

            # --- å…³é”®ä¿®æ”¹ï¼šæ›´ä¸¥æ ¼åœ°æå–å’Œæ¸…ç†ç»“æœ ---
            optimized = initial_translation  # é»˜è®¤å€¼

            # 1. ç§»é™¤ Prompt éƒ¨åˆ†ï¼šè¿™æ˜¯æœ€é‡è¦çš„ä¸€æ­¥
            # æ‰¾åˆ° Prompt æ–‡æœ¬åœ¨ Response ä¸­çš„ç»“æŸä½ç½®
            if response.startswith(prompt_text):
                optimized = response[len(prompt_text):].strip()
            else:
                optimized = response.strip()

            # 2. å½»åº•æ¸…ç†ï¼šç§»é™¤æ‰€æœ‰æ¢è¡Œç¬¦ã€å¼•å·ã€LLM ç‰¹æ®Šæ ‡è®°å’Œå¯èƒ½çš„é¢å¤–å¯¹è¯ã€‚

            # ç§»é™¤ LLM å¯èƒ½ç”Ÿæˆçš„é¢å¤–å¯¹è¯æ ‡è®°ï¼ˆä¾‹å¦‚ï¼šAssistant: æˆ– Human:ï¼‰
            # ä½¿ç”¨éè´ªå©ªåŒ¹é… .*?ï¼ŒåªåŒ¹é…åˆ°ç¬¬ä¸€ä¸ªæ¢è¡Œæˆ–ç»“æŸ
            optimized = re.sub(r'(Human:|Assistant:|\n\n).*', '', optimized, flags=re.IGNORECASE | re.DOTALL).strip()

            # ç§»é™¤ Qwen æ¨¡å‹çš„ç‰¹æ®Šæ ‡è®° <|endoftext|>
            optimized = optimized.replace("<|endoftext|>", "").strip()

            # ç§»é™¤æ‰€æœ‰å‰å¯¼/å°¾éšçš„å¼•å·
            optimized = optimized.strip('"').strip("'").strip()

            # ç§»é™¤æ‰€æœ‰æ¢è¡Œç¬¦ï¼Œå°†å¤šè¡Œæ–‡æœ¬åˆå¹¶ä¸ºä¸€è¡Œ
            optimized = optimized.replace('\n', ' ').strip()

            # 3. æœ€ç»ˆå®‰å…¨æ£€æŸ¥
            if not optimized or len(optimized) < 1:
                logger.warning(
                    f"Segment {segment_idx} reflection failed (empty/bad result), using initial translation.")
                return initial_translation

            logger.debug(f"Segment {segment_idx} reflection success: '{initial_translation}' -> '{optimized}'")
            return optimized

        except Exception as e:
            logger.error(f"Segment {segment_idx} reflection failed: {e}", exc_info=True)
            return initial_translation

    def _calculate_batch_qe_scores(self, source_texts: List[str], translated_texts: List[str]) -> List[float]:
        """æ‰¹é‡è®¡ç®—ç¿»è¯‘è´¨é‡ï¼ˆQEï¼‰åˆ†æ•°ï¼ˆåŸºäºå¥å­ç›¸ä¼¼åº¦ï¼‰"""
        if not source_texts or not translated_texts or len(source_texts) != len(translated_texts):
            logger.warning("Invalid input for QE score calculation.")
            return [0.0] * len(source_texts)

        try:
            src_embeddings = self.qe_model.encode(source_texts, convert_to_tensor=True, show_progress_bar=False)
            trans_embeddings = self.qe_model.encode(translated_texts, convert_to_tensor=True, show_progress_bar=False)

            similarities = util.cos_sim(src_embeddings, trans_embeddings).diag().cpu().numpy()

            qe_scores = [float(max(0, sim)) for sim in similarities]

            logger.info(f"Batch QE score calculation completed: avg_score={round(sum(qe_scores) / len(qe_scores), 2)}")
            return qe_scores

        except Exception as e:
            logger.error(f"QE score calculation failed: {e}", exc_info=True)
            return [0.0] * len(source_texts)

    def _get_lang_name(self, lang_code: str) -> str:
        """å°†è¯­è¨€ä»£ç è½¬æ¢ä¸ºä¸­æ–‡åç§°ï¼ˆç”¨äºæç¤ºè¯ï¼‰"""
        lang_names = {
            'zh': 'ä¸­æ–‡', 'zh-cn': 'ä¸­æ–‡', 'en': 'è‹±æ–‡', 'ja': 'æ—¥æ–‡', 'ko': 'éŸ©æ–‡',
            'fr': 'æ³•æ–‡', 'de': 'å¾·æ–‡', 'es': 'è¥¿ç­ç‰™æ–‡', 'ru': 'ä¿„æ–‡', 'ar': 'é˜¿æ‹‰ä¼¯æ–‡',
            'pt': 'è‘¡è„ç‰™æ–‡', 'it': 'æ„å¤§åˆ©æ–‡', 'nl': 'è·å…°æ–‡', 'pl': 'æ³¢å…°æ–‡'
        }
        return lang_names.get(lang_code.lower(), lang_code)

    def _cleanup_vram(self, nmt_only: bool = False):
        """æ¸…ç†æ¨¡å‹å ç”¨çš„æ˜¾å­˜/å†…å­˜"""
        if self.nmt_model:
            del self.nmt_model
            self.nmt_model = None

        if not nmt_only:
            if self.reflector:
                del self.reflector
                self.reflector = None
            if self.qe_model:
                del self.qe_model
                self.qe_model = None

        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("âœ… VRAM/CPU memory cleaned up successfully.")

    def get_supported_languages(self) -> Dict[str, List[str]]:
        """è¿”å›æ”¯æŒçš„è¯­è¨€åˆ—è¡¨ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰"""
        return {
            "whisper": ["auto", "en", "zh", "ja", "ko", "fr", "de", "es", "ru", "ar", "hi", "pt", "it", "nl", "pl"],
            "nmt": ["en", "zh", "zh-cn", "ja", "ko", "fr", "de", "es", "ru", "ar", "hi", "pt", "it", "nl", "pl"]
        }

    def __del__(self):
        """å¯¹è±¡é”€æ¯æ—¶è‡ªåŠ¨æ¸…ç†èµ„æº"""
        self._cleanup_vram()